# Uni Notes

Merhabalar,  

Bu bir intihar mekubudur ğŸ˜Š 

 Åaka ÅŸaka intoduction to machine learning dersine Ã§alÄ±ÅŸÄ±rken aldÄ±ÄŸÄ±m notlar. Dersin Ã¶zeti, hap bilgi olarak dÃ¼ÅŸÃ¼nebilirsiniz. 

![Alt text](<Screenshot 2024-05-22 at 07.45.58.png>)

Hocam makine Ã¶ÄŸrenmesi dediÄŸimiz zaman. Makielere Ã¶ÄŸretmekten bahsediyoroz. MutfaÄŸÄ±nÄ±zda hangi makine varsa blender olur mikro dalga olur alÄ±n karÅŸÄ±nÄ±za baÅŸlayan hayatÄ± Ã¶ÄŸretmeye ğŸ˜¬ yaptÄ±ÄŸÄ±m son iÄŸrenÃ§ espriydi. Ciddiyetle anlatÄ±yorum hemen 

 

Makine Ã¶ÄŸrenmesini Ã¼Ã§ kÄ±sÄ±ma aayÄ±rÄ±caz supervised(gÃ¶zetimli) leraning, unsupervised(gÃ¶zetimsiz) learning ve Reinforcement learning. Bu ders kapsamÄ±nda daha Ã§ok ilk ikisi Ã¼zerinde durucaz.  

###Â SUPERVÄ°SED LEARNÄ°NG 

Etiketli verilerin kullanÄ±ldÄ±ÄŸÄ± modellerdir. Modeli eÄŸitmek iÃ§in kullandÄ±ÄŸÄ±mÄ±z veri etiketli olur. Zaten bu yÃ¼zden supervised yani gÃ¶zetimli Ã¶ÄŸrenme diyoruz. VerdiÄŸimiz verinin sÄ±nÄ±flandÄ±rmasÄ± (etiketlenmesi) insan gÃ¶zetimi altÄ±nda istediÄŸimz gib, sÄ±nÄ±flayÄ±p veriyoruz veriyi. Bu yÃ¼zdende supervised learning iÃ§in â€œ such as an input  where desired output is knonwâ€ tam olarka mevzu gÃ¶zetimli Ã¶ÄŸrenmede zaten veriyi sÄ±nÄ±flandÄ±rÄ±p laballayÄ±p. Sonrada modeli hazÄ±r bizim sÄ±nÄ±flandÄ±rdÄ±ÄŸÄ±mÄ±z veryle eÄŸittiÄŸimizde isteÄŸimz Ã§Ä±ktÄ±yÄ±. Yani yenÅŸ veriyi nasÄ±l nereye koymasÄ±nÄ± istediÄŸimizi zaten Ã§ok iyi biliyoruz.  

### Unsupervised Learning 

Geldik zor kÄ±sma. Hocam yukarÄ±dakinden mevzuya biraz uyandÄ±k zaten supervised de labellÄ± datayÄ± training date set olark kullanÄ±yorsak unsupervised learning de labelsÄ±z veriyi once sÄ±nÄ±flandÄ±rÄ±p sonr atraining set olarak kullanÄ±caz. Yani labellama-sÄ±nÄ±flandÄ±rma kÄ±smÄ±da makineye kalÄ±cak. GÃ¶zetimsiz Ã¶ÄŸrenmeyi karmaÅŸÄ±k yapan kÄ±sÄ±mda aslÄ±nda eÄŸitim setnde etiketi olmayan veriyi nasÄ±l sÄ±nÄ±flandÄ±racaÄŸÄ±mÄ±z. iIleride bunun Ã¼zerinde bol bol durucaz. 


![Alt text](<Screenshot 2024-05-22 at 07.46.24.png>)

## TERMÄ°NOLOJÄ° 

Hocam ÅŸimdi lineaer regresyon Ã¼zerinden anlatÄ±cam kavramlarÄ± Ã§Ã¼nkÃ¼ daha kolay oluyor. 

Y=ax1+a2x2+a3x3+- - - - +anxn 

Label : label dediÄŸimiz ÅŸey etiket. Biz modeli eÄŸittikten sonra yeni bir veri verip bu verinin yerini yani etiketin bulmaya Ã§alÄ±ÅŸmÄ±yor muyuz? Yani label dediÄŸimiz ÅŸey denklemde y ye takabÃ¼l ediyor. 

Feature : bunlar verinin Ã¶zellikleri. SÄ±nÄ±flandÄ±rmak iÃ§in kullanÄ±lan Ã¶zellikler yani. NasÄ±l y yi bulmak iÃ§in x deÄŸikenÅŸne veriden deÄŸer etkiliyorsa yanÅŸ x lerle iÅŸlem yapÄ±p y yi bulÄ±yorsak veride de feture yani Ã¶zelliklerine bakÄ±p labelâ€™Ä±nÄ± buluyoruz. Mesela bir yapraÄŸÄ±n tÃ¼rÃ¼nÃ¼ bulmak istresdiÄŸimizi dÃ¼ÅŸÃ¼nelim. YapraÄŸÄ±n genilÄŸiÅŸi, rengi, kalÄ±nlÄ±Ä± feature olur yani X. yapraÄŸÄ±n tÃ¼rÃ¼ de y yani label olur ğŸ˜Š hadi ek bilgi y deÄŸerini bulmak iÃ§in daah Ã¶nemli olan featureâ€™lara yani x deÄŸerlerine dah yÃ¼ksek katsayÄ± veririz. Yani modeli eÄŸitirken Ã¶nemli olan Ã¶zelliklerin aÄŸÄ±rlÄ±ÄŸÄ±nÄ± narttÄ±rÄ±caz yani dah yÃ¼ksek a deÄŸeri vericez ğŸ˜Š 

### Regresyon ve Classification 

Regresyonda sÃ¼rekli deÄŸerlerle ilgilenir. Bu reklema tÄ±klanma ihtimali. KarabÃ¼kte 10 yÄ±l sonra  bulunacak ev sayÄ±sÄ±nÄ±n tahmini vb. 

SÄ±nÄ±flandÄ±rma ise sÃ¼reksiz deÄŸerler bakar e-mail spam yada deÄŸil. Bu cisim kÃ¶pek, ekmek yada domuz gibi. Burada sÃ¼reksizden kastÄ±mÄ±z 0-1 olmasÄ± sÃ¼rekliden kastÄ±mÄ±z normal sayÄ± olmasÄ±. Yani grafiÄŸe baktÄ±ÄŸÄ±n sÄ±nÄ±flandÄ±rma 0-1 ÅŸeklinde olacaÄŸÄ± iÃ§in kesikli bir grafik elde edilir. DiÄŸerinde ise sayÄ± olcaÄŸÄ± iÃ§in sÃ¼rekli grafik olur. SÃ¼rekli sÃ¼reksiz muhabbeti buradan gelir. 

-> BaÅŸlÄ±ca SÄ±kÄ±ntÄ±lar 

BÃ¶yle baÅŸlÄ±k mÄ± olur? Hocam ilerde Ã¼zerinde bol bo duracaÄŸÄ±mÄ±z mlâ€™de sÄ±k karÅŸÄ±lalaÅŸÄ±lan sorunlara bir Ã¼sten gÃ¶z atalÄ±m. Rakibe bi selam verelim gibi 

* Insufficient Quantity of trainin data: yani veri miktarÄ± yetersizliÄŸi 
* Nonrepresentetive data: temsili olmayan veri. Elimizdeki verinin geneli temsil etmesi gerekir. EÄŸer verimiz Ã§ok az olursa gÃ¼rÃ¼ltÃ¼ olma ihtimali gÃ¼rÃ¼ltÃ¼den etkilenme ihtimali daha yÃ¼ksktir. Ancak data nÄ±n Ã§ok olmasÄ± olmasÄ± bizi temsil sorunundan direkt kurtarmak eÄŸer Ã¶rnekleme metodund ahata varsa yine verimiz yanlÄ± olabilir(samling bias) 
* Poor qualty data: hocam eÄŸer verimizde yanlÄ±ÅŸ Ã¶lÃ§Ã¼m nedeiyle bol bol error(hata), outliers(aykÄ±rÄ± deÄŸer), noise(gÃ¼rÃ¼ltÃ¼) varsa bu durumda verimizi temizlememiz gerekir. Bu durumda ya bu Ã¶zelliÄŸi gÃ¶rmezden geliriz ya boÅŸ kÄ±sÄ±mlarÄ± median(ortanca) ile doldururuz. 
* Irrelevant features: ilgisiz Ã¶zellikler. Hocam burada feature selection(Ã¶zellik seÃ§imi) ve feature extraction(Ã¶zellik Ã§Ä±karma) durumlarÄ± ortaya Ã§Ä±kÄ±yor. Iyi veris seti koy abicim koy bu Ã¶zelliÄŸi de ne bulursan ekle diyerek Ã§Ä±kmaz. Elimizdeki Ã¶zelliklerden iyi,  gerekli olanlarÄ± seÃ§mek ve gereksiz olanlarÄ± Ã§Ä±karmalÄ±yÄ±z. 
* Overfitting: makinemiz ÄŸitim setine mÃ¼kemmel uyum saÄŸlar ve Ã§ok yÃ¼ksek oranda doÄŸruluk oranÄ±nan sahiptir. Ancak baÅŸk averi verildiÄŸinde baÅŸarÄ± ciddi ÅŸekilde dÃ¼ÅŸer.  
* Underfitting :overfittingin tersi oluyor. Modelimiz basit kaÃ§arsa olur. Yine doÄŸruluk oranÄ± haliyle dÃ¼ÅŸÃ¼k olur. Modelimiz baÅŸarÄ±lÄ± olamaz. Bunun iÃ§in data ile deÄŸil modelimiz ile uÄŸraÅŸmamÄ±z gerekir. Daha Ã§ok parametreli bir model seÃ§ebiliriz yada model Ã¼zerindeki kÄ±sÄ±tlamalarÄ± azaltabiliriz vb. 
 
### Testing And Validation 
Hocam veri setinin tamamÄ±yla modeli eÄŸitmessin Ã¶nce bir kÄ±smÄ±nÄ± valide (doÄŸrulama) etmek iÃ§in yani modelini test iÃ§in ayÄ±rÄ±rsÄ±n. YanÅŸ verinin bir kÄ±smÄ±yla model eÄŸitilir kalan kÄ±smÄ±yla valide edilir.  
 
### Hyperparameter Tuning 
--bu kÄ±sÄ±m daha sonra eklenecektir-- 
 